{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75e08a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1553dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\n",
    "    \"F:/Users/elect_09l/github/ship_detection/YOLO_MODEL\"\n",
    ")  # Set the base directory of the script and where other files will be created\n",
    "\n",
    "dir_base = r\"F:/Users/elect_09l/github/ship_detection\"\n",
    "dir_yolo = dir_base + \"/YOLO_MODEL/\"\n",
    "\n",
    "dir_folder_list = [\n",
    "    dir_yolo + \"analysed_imgs\"\n",
    "]\n",
    "\n",
    "folder_list = [\"analysed_imgs\"]\n",
    "\n",
    "if BOOL_DELETE_DIR:\n",
    "    delete_dir()\n",
    "if BOOL_MAKE_DIR:\n",
    "    make_dir()\n",
    "\n",
    "\n",
    "# Load YOLO Model\n",
    "net = cv2.dnn.readNet(\n",
    "    \"D:/py/project/YOLO/\" + \"yolov3.weights\", dir_yolo + \"yolov3.cfg\"\n",
    ")  # load weights dir and configurations dir\n",
    "\n",
    "classes = []\n",
    "with open(dir_yolo + \"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "line_pts = (\n",
    "    []\n",
    ")  # List of all the points of detection by the model, tuples are added through each iteration of the loop below\n",
    "\n",
    "first_iter = True\n",
    "\n",
    "# Settings for the \"Start Point\" (shows the beginning of the path in which the ship moves in)\n",
    "start_pt_radius = 15\n",
    "start_pt_colour = (23, 144, 255)\n",
    "start_pt_centre = (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "392c25de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./input/download.jpg\n"
     ]
    }
   ],
   "source": [
    "# read input image\n",
    "img = \"./input/download.jpg\"\n",
    "print(img)\n",
    "image = cv2.imread(img)\n",
    "\n",
    "Width = image.shape[1]\n",
    "Height = image.shape[0]\n",
    "scale = 0.00392\n",
    "\n",
    "img_size = (Width, Height)\n",
    "\n",
    "# read class names from text file\n",
    "# generate different colors for different classes \n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# create input blob \n",
    "blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
    "\n",
    "# set input blob for the network\n",
    "net.setInput(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0680caf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d4f1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the output layer names \n",
    "# in the architecture\n",
    "def get_output_layers(net):\n",
    "    \n",
    "    layer_names = net.getLayerNames()\n",
    "    \n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "# function to draw bounding box on the detected object with class name\n",
    "def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "\n",
    "    label = str(classes[class_id])\n",
    "\n",
    "    color = COLORS[class_id]\n",
    "\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
    "\n",
    "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bcd41af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference through the network\n",
    "# and gather predictions from output layers\n",
    "outs = net.forward(get_output_layers(net))\n",
    "\n",
    "# initialization\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "conf_threshold = 0.5\n",
    "nms_threshold = 0.4\n",
    "\n",
    "# for each detetion from each output layer \n",
    "# get the confidence, class id, bounding box params\n",
    "# and ignore weak detections (confidence < 0.5)\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * Width)\n",
    "            center_y = int(detection[1] * Height)\n",
    "            w = int(detection[2] * Width)\n",
    "            h = int(detection[3] * Height)\n",
    "            x = center_x - w / 2\n",
    "            y = center_y - h / 2\n",
    "            class_ids.append(class_id)\n",
    "            confidences.append(float(confidence))\n",
    "            boxes.append([x, y, w, h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcd025d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply non-max suppression\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "# go through the detections remaining\n",
    "# after nms and draw bounding box\n",
    "for i in indices:\n",
    "    i = i[0]\n",
    "    box = boxes[i]\n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "    \n",
    "    draw_bounding_box(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
    "\n",
    "FILENAME = classes[class_ids[i]]\n",
    "    \n",
    "# display output image    \n",
    "cv2.imshow(\"object detection\", image)\n",
    "\n",
    "# wait until any key is pressed\n",
    "cv2.waitKey()\n",
    "    \n",
    " # save output image to disk\n",
    "cv2.imwrite(f\"output/{FILENAME}.jpg\", image)\n",
    "\n",
    "# release resources\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59407618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "print(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f71276d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boat'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(classes[class_ids[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4127e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948eb3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b755198",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "61c795c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'cv2.dnn_Net' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-5794a4979ac9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvisualize_class_activation_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output/werewrew.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-58-92b67fc489b8>\u001b[0m in \u001b[0;36mvisualize_class_activation_map\u001b[1;34m(model_path, img_path, output_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m        \u001b[1;31m#Get the 512 input weights to the softmax.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m        \u001b[0mclass_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m        \u001b[0mfinal_conv_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_output_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"conv5_3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m        get_output = K.function([model.layers[0].input], \\\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'cv2.dnn_Net' object has no attribute 'layers'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2333d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f9acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
